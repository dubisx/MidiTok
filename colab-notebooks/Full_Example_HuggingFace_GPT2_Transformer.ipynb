{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Full example with the Hugging Face Transformers package\n",
    "\n",
    "This notebook shows how to train a model (GPT2) and generate music from it, using the Hugging Face Transformers package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple, Dict, Callable, Any, Union\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from torch import Tensor, LongTensor, stack, flip, cat, full, argmax\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtoolkit.data import create_subsets\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from miditok import REMI, MIDITokenizer\n",
    "from miditok.constants import CHORD_MAPS\n",
    "from miditoolkit import MidiFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    r\"\"\"Dataset for generator training\n",
    "\n",
    "    :param files_paths: list of paths to files to load.\n",
    "    :param tokenizer: tokenizer object, to use to load MIDIs instead of tokens. (default: None)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, files_paths: List[Path], min_seq_len: int, max_seq_len: int, tokenizer: MIDITokenizer = None):\n",
    "        samples = []\n",
    "\n",
    "        for file_path in tqdm(files_paths, desc=f'Loading data: {files_paths[0].parent}'):\n",
    "            if file_path.suffix in [\"mid\", \"midi\", \"MID\", \"MIDI\"]:\n",
    "                midi = MidiFile(file_path)\n",
    "                for _ in range(len(midi.instruments) - 1):\n",
    "                    del midi.instruments[1]  # removes all tracks except first one\n",
    "                tokens = tokenizer.midi_to_tokens(midi)[0].ids\n",
    "            else:\n",
    "                with open(file_path) as json_file:\n",
    "                    tokens = json.load(json_file)['ids'][0]  # first track\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i >= len(tokens) - min_seq_len:\n",
    "                    break  # last sample is too short\n",
    "                samples.append(LongTensor(tokens[i:i + max_seq_len]))\n",
    "                i += len(samples[-1])  # could be replaced with max_seq_len\n",
    "\n",
    "        self.samples = samples\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, LongTensor]:\n",
    "        return {\"input_ids\": self.samples[idx], \"labels\": self.samples[idx]}\n",
    "    \n",
    "    def __len__(self) -> int: return len(self.samples)\n",
    "\n",
    "    def __repr__(self): return self.__str__()\n",
    "\n",
    "    def __str__(self) -> str: return 'No data loaded' if len(self) == 0 else f'{len(self.samples)} samples'\n",
    "\n",
    "\n",
    "def _pad_batch(examples: List[Dict[str, LongTensor]], pad_token: int) -> LongTensor:\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "\n",
    "    length_of_first = examples[0][\"input_ids\"].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    are_tensors_same_length = all(x[\"input_ids\"].size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length:\n",
    "        return stack([e[\"input_ids\"] for e in examples], dim=0).long()\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    return pad_sequence([e[\"input_ids\"] for e in examples], batch_first=True, padding_value=pad_token).long()\n",
    "\n",
    "\n",
    "class DataCollatorGen(DataCollatorMixin):\n",
    "    def __init__(self, pad_token: int, return_tensors: str = \"pt\"):\n",
    "        \"\"\"Collator that simply pad the input sequences.\n",
    "        Input_ids will be padded with the pad token given, while labels will be\n",
    "        padded with -100.\n",
    "\n",
    "        :param pad_token: pas token\n",
    "        :param return_tensors:\n",
    "        \"\"\"\n",
    "        self.pad_token = pad_token\n",
    "        self.return_tensors = return_tensors\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]], return_tensors=None) -> Dict[str, LongTensor]:\n",
    "        x, y = _pad_batch(batch, self.pad_token), _pad_batch(batch, -100)\n",
    "        return {\"input_ids\": x, \"labels\": y}  # will be shifted in GPT2LMHead forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MIDI files to tokens, and load them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDIs (mini data/tokens_noBPE): 100%|██████████| 12/12 [00:00<00:00, 70.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Our parameters\n",
    "pitch_range = range(21, 109)\n",
    "beat_res = {(0, 4): 8, (4, 12): 4}\n",
    "nb_velocities = 32\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True,\n",
    "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (40, 250),  # (min, max)\n",
    "                     'Program': False,\n",
    "                     \"chord_maps\": CHORD_MAPS,\n",
    "                     \"chord_tokens_with_root_note\": True,\n",
    "                     \"chord_unknown\": False}\n",
    "special_tokens = [\"PAD\", \"BOS\", \"EOS\"]\n",
    "\n",
    "# Creates the tokenizer convert MIDIs to tokens\n",
    "tokens_path = Path(\"C:/Users/simas/OneDrive/Desktop/alawais midi/mini data/tokens_noBPE\")\n",
    "tokenizer = REMI(pitch_range, beat_res, nb_velocities, additional_tokens, special_tokens=special_tokens) # REMI\n",
    "midi_paths = list(Path(\"C:/Users/simas/OneDrive/Desktop/alawais midi/mini data/midi\").glob('**/*.mid'))\n",
    "tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: C:\\Users\\simas\\OneDrive\\Desktop\\alawais midi\\mini data\\tokens_BPE: 100%|██████████| 96/96 [00:00<00:00, 233.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_paths = list(Path(\"C:/Users/simas/OneDrive/Desktop/alawais midi/mini data/tokens_BPE\").glob(\"**/*.json\"))\n",
    "dataset = MIDIDataset(\n",
    "    tokens_paths, max_seq_len=512, min_seq_len=384, \n",
    ")\n",
    "subset_train, subset_valid = create_subsets(dataset, [0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000002DBE869B1F0> <torch.utils.data.dataset.Subset object at 0x000002DBE869B3D0>\n"
     ]
    }
   ],
   "source": [
    "print(subset_train, subset_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "We will use the [GPT2 implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2). This \n",
    "Feel free to explore the documentation and source code to dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=2048,\n",
    "    n_embd=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_inner=2048,\n",
    "    resid_pdrop=.1,\n",
    "    embd_pdrop=.1,\n",
    "    attn_pdrop=.1,\n",
    "    padding_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None'],\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26368/941347033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mtrain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Saves the tokenizer too\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\simas\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         )\n\u001b[1;32m-> 1633\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\simas\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;31m# Data loader and number of training steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m         \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m         \u001b[1;31m# Setting up training control variables:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\simas\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    879\u001b[0m             )\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m         \u001b[0mtrain_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_train_sampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m         return DataLoader(\n",
      "\u001b[1;32mc:\\Users\\simas\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m             elif (\n\u001b[0;32m    825\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mParallelMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAGEMAKER_MODEL_PARALLEL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\simas\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[0;32m    108\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "metrics = {metric: load_metric(metric) for metric in [\"accuracy\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes metrics for pretraining.\n",
    "    Must use proprocess_logits function that converts logits to predictions (argmax or sampling).\n",
    "\n",
    "    :param eval_pred: EvalPrediction containing predictions and labels\n",
    "    :return: metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    not_pad_mask = labels != -100\n",
    "    labels, predictions = labels[not_pad_mask], predictions[not_pad_mask]\n",
    "    return metrics[\"accuracy\"].compute(predictions=predictions.flatten(), references=labels.flatten())\n",
    "\n",
    "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
    "    \"\"\"Preprocesses the logits before accumulating them during evaluation.\n",
    "    This allows to significantly reduce the memory usage and make the training tractable.\n",
    "    \"\"\"\n",
    "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
    "    return pred_ids\n",
    "\n",
    "training_config = TrainingArguments(\n",
    "    \"runs\", False, True, True, False, \"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=48,\n",
    "    gradient_accumulation_steps=3,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=3.0,\n",
    "    max_steps=100000,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.3,\n",
    "    log_level=\"debug\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    no_cuda=False,\n",
    "    seed=444,\n",
    "    load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    data_collator=DataCollatorGen(tokenizer[\"PAD_None\"]),\n",
    "    train_dataset=subset_train,\n",
    "    eval_dataset=subset_valid,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits,\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "def collate_gen_left(batch: List[Dict[str, LongTensor]]) -> LongTensor:\n",
    "    # Here the sequences are padded to the left, so that the last token along the time dimension\n",
    "    # is always the last token of each seq, allowing to efficiently generate by batch\n",
    "    bos_shape = (1,)\n",
    "    batch = [flip(cat([full(bos_shape, tokenizer[\"BOS_None\"]), seq[\"input_ids\"]], dim=0), dims=(0,)) for seq in batch]\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=tokenizer[\"PAD_None\"])  # (N,T) or (N,T,Z)\n",
    "    batch = flip(batch, dims=(1,)).long()\n",
    "    return batch  # (N,T)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,  # extends samples by 512 tokens\n",
    "    num_beams=1,        # no beam search\n",
    "    do_sample=True,     # but sample instead\n",
    "    temperature=0.9,\n",
    "    top_k=15,\n",
    "    top_p=0.95,\n",
    "    epsilon_cutoff=3e-4,\n",
    "    eta_cutoff=1e-3,\n",
    "    pad_token_id=config.padding_token_id,\n",
    ")\n",
    "\n",
    "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
    "dataloader_test = DataLoader(subset_valid, batch_size=16, collate_fn=collate_gen_left)\n",
    "model.eval()\n",
    "count = 0\n",
    "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):  # (N,T)\n",
    "    res = model.generate(batch.to(model.device), generation_config=generation_config)  # (N,T)\n",
    "\n",
    "    # Saves the generated music, as MIDI files and tokens (json)\n",
    "    for prompt, continuation in zip(batch, res):\n",
    "        generated = continuation[len(prompt):]\n",
    "        tokens = [generated, prompt, continuation]  # list compr. as seqs of dif. lengths\n",
    "        tokens = [seq.tolist() for seq in tokens]\n",
    "        midi = tokenizer.tokens_to_midi(deepcopy(tokens), time_division=384)\n",
    "        midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
    "        midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
    "        midi.instruments[2].name = f'Original sample and continuation'\n",
    "        midi.dump(gen_results_path / f'{count}.mid')\n",
    "        tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json')   \n",
    "\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
